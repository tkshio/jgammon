:toc: macro
:sectnums:
:toc-title: 目次


toc::[]

[[usage]]
== 使用法

訓練と対局の2つのサブコマンドが使用できます。省略時は、対局コマンドが実行されます。

=== 訓練コマンド

訓練では、訓練対局と記録用の対局（記録あり対局）を繰り返します。対局（いずれの種類も）により、学習が行われます。

以下の例では、10回訓練対局を行い、次に3回記録あり対局を行います。

----

> java -jar jgammon-1.0-SNAPSHOT-all.jar training 10 3

...
2020-06-29 16:34:49   -2  0  0  0 -1  0       0 -1  0 -1  0  0
2020-06-29 16:34:49  +24-23-22-21-20-19------18-17-16-15-14-13-+
2020-06-29 16:34:49  | O           O    |   |    O     O       |
2020-06-29 16:34:49  | O                |   |                  |
2020-06-29 16:34:49  |                  |   |                  |
2020-06-29 16:34:49  |                  |   |                  |
2020-06-29 16:34:49  |                  |   |                  |
2020-06-29 16:34:49 v|                  |BAR|                  |
2020-06-29 16:34:49  |                  |   |                  |
2020-06-29 16:34:49  |                  |   |    O  O          |
2020-06-29 16:34:49  |                  |   |    O  O          |
2020-06-29 16:34:49  |                  |   |    O  O          |
2020-06-29 16:34:49  |          O       |   | O  O  O          |
2020-06-29 16:34:49  +-1--2--3--4--5--6-------7--8--9-10-11-12-+
2020-06-29 16:34:49    0  0  0 -1  0  0      -1 -4 -4  0  0  0
2020-06-29 16:34:49
2020-06-29 16:34:49
2020-06-29 16:34:49
2020-06-29 16:34:49 #3          red won by Backgammon.   3 - 3   ( 358 plys)
2020-06-29 16:34:49
2020-06-29 16:34:49 Trials done. red:3 - white:3 , 2 game(s)
2020-06-29 16:34:49             red single:0 g:0 bg:1 (s:  0.0% b:  0.0% bg: 50.0% , win: 50.0% = s:  0.0% + g,bg: 50.0%)
2020-06-29 16:34:49           white single:0 g:0 bg:1 (s:  0.0% b:  0.0% bg: 50.0% , win: 50.0% = s:  0.0% + g,bg: 50.0%)
2020-06-29 16:34:49
2020-06-29 16:34:49 Plys statistics:IntSummaryStatistics{count=2, sum=413, min=55, average=206.500000, max=358}

Trials done. red:3 - white:3 , 2 game(s)
            red single:0 g:0 bg:1 (s:  0.0% b:  0.0% bg: 50.0% , win: 50.0% = s:  0.0% + g,bg: 50.0%)
          white single:0 g:0 bg:1 (s:  0.0% b:  0.0% bg: 50.0% , win: 50.0% = s:  0.0% + g,bg: 50.0%)

Plys statistics:IntSummaryStatistics{count=2, sum=413, min=55, average=206.500000, max=358}

----

訓練対局では、標準出力に全体結果が出力される以外、ほかの情報はほとんど出力されません。記録あり対局では、エラー出力に経過が詳細に出力されます。

引数が2つ以上ある場合も、順次訓練対局数、記録あり対局数として繰り返し対局が行われます。

以下の例では、10回訓練→3回記録→100回訓練→10回記録という順で対局を行います。

----

java -jar jgammon-1.0-SNAPSHOT-all.jar training 10 3 100 10

----

==== 出力オプション

===== 学習結果の入出力

* ``--out [ファイル名]`` を指定することにより、学習したニューラルネットワークの状態を出力します。
* ``--in [ファイル名]`` で出力されたファイルを指定することにより、ファイルに記録された状態から学習を再開します。

===== ログ出力

* ``--log [ファイル名]`` を指定することにより、標準エラーに出力される内容を指定のファイルに出力します。

===== 棋譜の出力

* ``--xg [ファイル名]`` を指定することにより、テキスト形式の棋譜を出力します（対象は記録あり対局のみ）。棋譜は、Extreme Gammonで読み込ませることが可能です。

===== 経過出力

* ``-v`` を指定することにより、訓練対局では各ゲームの結果が出力されるようになり、進行が把握できるようになります。

==== 学習オプション

* ``--lambda [0-1の範囲の値]`` により、Temporal-Difference法のλパラメーターを指定できます。デフォルトは0.7で、これはTD-Gammonで採用された値です。0にすると直前の局面の評価の影響が弱まり、1にすると強まります。

* ``--learning_rate [0-1の範囲の値]`` により、ニューラルネットワークの調整時に使用する係数を指定できます。デフォルトは0.01で、これもlambda同様TD-Gammonで採用された値です。小さくするほど一回の学習ごとの調整量が小さくなります。

* ``--node [ノード数]`` により、隠れ層のノード数を指定できます。デフォルトは200です。TD-Gammonではバージョンにより40、または80ノードとしています。

==== そのほかのオプション

* ``--max [ターン数]`` 1対局当たりのターン数を制限します。デフォルトは200です。両プレイヤーが一回ずつ手番を行うのが1ターン（2ply）です。 +
バックギャモンの場合平均は30ターン(60ply)程度で、1000回-10000回ほど学習が進むとその範囲に収まるようになりますが、学習初期では数十万ターンに達することがあります。

* ``--names [O側の名前] [X側の名前]``  出力時の名前を指定します（O=white/X=redのままの個所もあります）。


* ``--seed [シード値]`` 乱数生成に用いるシード値を指定することで、再現性のある試行が可能です。省略時は乱数で決定され、ログの先頭に出力されます。

=== 対局コマンド

指定したアルゴリズム同士で対局を行います。とはいえ、ニューラルネットワークによる評価アルゴリズム（``td``）の他には、ランダムアルゴリズム（``random``）しか実装されていません。

対局時は以下のように、``--alg`` を用いて O、Xそれぞれのアルゴリズムと回数を指定します（Oが``td``、相手は``random``）。trainコマンド同様、引数を並べることが可能で、以下の例では7回の対局を2回行います（こちらのコマンドでは全て記録あり対局になります）。

----
> java -jar jgammon-1.0-SNAPSHOT-all.jar run --alg td random 7 7
----

``--alg`` オプション自体を省略すると、デフォルトでは``td`` アルゴリズムが使用されます。これは、100万回の自己対局を終えたニューラルネットワークです。

==== 学習済ファイルの使用

以下のように、アルゴリズムに ``--td:[ファイル名]``を指定することで、訓練により生成したファイルを使用することができます。なお、学習は行いません。

----
> java -jar jgammon-1.0-SNAPSHOT-all.jar run --alg td:out/dump.txt td:out/dump2.txt 7 7
----

==== 読みの深さの指定

対局コマンドでは、 ``--depth`` により読みの深さを1か2で指定できます。以下のようにO,Xそれぞれの深さを並べて指定します。

----
> java -jar jgammon-1.0-SNAPSHOT-all.jar run --depth 1 2  7 7
----

この例では、white側は1ply、すなわち自分の手を指した後の状態を評価して手を選択します。red側は2plyで、さらに相手のロールと手をすべて列挙し、それらの評価により手を選択します。

当然ながら2plyの指定はランダムアルゴリズムには全く意味がなく、 ``td`` アルゴリズムでのみ有効です。また、1ply/2ply以外の設定はありません。なお、訓練中は常に1plyの評価です。

== 実行例・性能について

=== 実行例

以下のような感じで実行します。

.1. 訓練の実施
--
記録を挟みつつ、10万局の訓練を三回行います

----

> java -jar jgammon-1.0-SNAPSHOT-all.jar train --out out/dump.txt --log out/training.log 100000 10 100000 10 100000 10

----
--

.2. 対局
--
訓練したニューラルネットワークと内蔵のニューラルネットワークとで対局し、結果をXGで分析します

----

> java -jar jgammon-1.0-SNAPSHOT-all.jar run -v --alg td td:out/dump.txt --log out/300kVSdefault.log --xg out/300kVSdefault.txt 10

----
--

.3. 読みの深さを変えて対局
--
自分自身で1ply-2ply対局を行います

----

> java -jar jgammon-1.0-SNAPSHOT-all.jar run -v --alg td:out/dump.txt td:out/dump.txt --log out/1plyVS2ply.log --xg out/1plyVS2ply.txt 10

----
--

=== 処理時間

効率や性能は度外視で開発したので、さして速くはないと思われます。 手元環境（Intel Core i7-8700 @ 3.20G、6コア）では、10万局の学習で100分ほどかかります(隠れ層のノード数は標準の80）。

TD-Gammonの初期バージョンでは30万局対局したとのことなので、その程度なら半日仕事ということになります。

また2plyの対局は1plyで比べるとかなり遅くなります。同環境では100局こなすのに1ply同士なら8.2秒のところ、2ply同士では450秒と約50倍の差が出ました。

=== 強さ

デフォルトのエンジンは隠れ層200ノードで、100万対局させてあります。100局の自己対戦譜をXGに評価させたところ、2Plyはエキスパート、1Plyは中級者といったところです。

ゲームの展開によってはかなり良いスコアを出しますが、私が見ても間違いとわかるようなプレイも見受けられ、単純な学習エンジンの限界が伺えます。

=== ND4J

行列演算にND4Jを使用することも可能ですが、手元で確認した限りではかえって遅い（マトリクスの規模が小さいため？）ため、切り離してあります（tdlearnND4Jサブプロジェクト）。

[discrete]
== 実装していないこと

* キューブアクション、ポイントマッチ
* 対人インターフェース
* 任意の局面の評価 → 処理自体は単体テストコード中にありますが（trainingプロジェクト）、UIが必要です
* ロールアウト → 力押しなら容易に実現可能ですが、実用上は https://en.wikipedia.org/wiki/Variance_reduction[Variance Reduction] をきちんとやりたいところ
* 枝刈り → 現在は2plyの読みしかしていないので省いていますが、より深く読むには必要でしょう
* 性能の向上・Deep Learning化 → 今回はアルゴリズムの理解が主目的だったので……
